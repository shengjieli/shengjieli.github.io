
# url编码原理

上篇文章说了`Python2.x`中一个奇怪的url编码问题。这次我们用`Python`的`urllib`库源码来看下在Web开发和爬虫中经常要用到的url编码和解码的原理。
### 为什么要用url编码？
在网络标准RFC 1738中规定：
> Thus, only alphanumerics, the special characters "$-_.+!*'(),", and reserved characters used for their reserved purposes may be used unencoded within a URL.

也就是只有字母、数字、一些特殊符号和保留字才可以不经过编码直接用于url中。所以，如果url中包含中文或一些特殊的符号就必须要经过url编码。

### url编码原理
先看一个url编码的例子
```
In [11]: import urllib
In [12]: s = u'小黑'
In [13]: urllib.quote(s.encode('utf-8'))
Out[13]: '%E5%B0%8F%E9%BB%91'
In [14]: urllib.quote(s.encode('gbk'))
Out[14]: '%D0%A1%BA%DA'
```
同一个中文字符串用`utf-8`和`gbk`编码后进行url编码竟然得出了不同的结果。这是为什么呢？
我们看下`Python`的`urllib.quote`的源码：
```python
always_safe = ('ABCDEFGHIJKLMNOPQRSTUVWXYZ'
                    'abcdefghijklmnopqrstuvwxyz'
                    '0123456789' '_.-')
_safe_map = {}
for i, c in zip(xrange(256), str(bytearray(xrange(256)))):
    _safe_map[c] = c if (i < 128 and c in always_safe) else '%{:02X}'.format(i)
_safe_quoters = {}
def quote(s, safe='/'):
    if not s:
        if s is None:
            raise TypeError('None object cannot be quoted')
        return s
    cachekey = (safe, always_safe)
    try:
        (quoter, safe) = _safe_quoters[cachekey]
    except KeyError:
        safe_map = _safe_map.copy()
        safe_map.update([(c, c) for c in safe])
        quoter = safe_map.__getitem__
        safe = always_safe + safe
        _safe_quoters[cachekey] = (quoter, safe)
if not s.rstrip(safe):
    return s
return ''.join(map(quoter, s))
```
这就是`Python`中url编码函数的源码。这段代码首先定义了一个包含大小写字母，数字和`_.-`三个字符的字符串，然后定义了一个`_safe_map`字典，字典的key是从0-255的字符，如果这个字符的`ascii`码值小于128且这个字符在`always_safe`字符串中，字典的值就是这个字符，否则，字典的值就是`%`加上这个字符的`ascii`码的16进制的值（不足两位的前边补0）。
再看下`quote`函数。`quote`函数先创建了一个由`safe`参数和`always_safe`参数组成的元组`cachekey`，这个主要是为了做缓存，如果多此用到这个函数就不用每次都生成`safe_map`和`quote`这个函数。它又把`safe`里的字符串更新到之前生成的`_safe_map`字典中，并且字典的值是`safe`里的字符，也就是说`safe`里的每个字符都不需要被编码。最后它把字符串里的每个字符用`safe_map`的键替换为值返回。
看到这里，我想你应该已经明白了：url编码就是用`%`加上字符的`ascii`码的16进制值来替换掉字符串中的一些不可见的，特殊的字符。
回到上边的那个用不同编码方式得出的url编码的字符串不同的问题上，应该就很容易知道这是因为不同编码方式编码后的字节数组(Python中就是str类型的字符换)是不一样的，所以导致url编码后的字符串不同。看下下面的代码就更清晰了。
```
In [37]: import urllib
In [38]: s = u'小黑'
In [39]: s.encode('utf-8')
Out[39]: '\xe5\xb0\x8f\xe9\xbb\x91'
In [40]: urllib.quote(s.encode('utf-8'))
Out[40]: '%E5%B0%8F%E9%BB%91'
In [41]: s.encode('gbk')
Out[41]: '\xd0\xa1\xba\xda'
In [42]: urllib.quote(s.encode('gbk'))
Out[42]: '%D0%A1%BA%DA'
```
这里只从url编码的源码说了编码的原理，解码的也是一样的原理，感兴趣的可以自己去看下。
在`Python`中url编码还有另外一个函数`urllib.quote_plus`，这个函数只是对空格这个字符做了单独的处理，这个字符使用`urllib.quote_plus`会被转成`+`这个字符，其他的都和`urllib.quote`一样。

