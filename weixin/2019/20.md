# 分布式任务队列Celery

今天我们来介绍下Python下的分布式任务队列Celery。

github地址：https://github.com/celery/celery

Celery是一个使用Python编写的简单、快速、灵活、高可用的一个分布式任务工具，支持异步任务和定时任务。并且可以很方便的和Django和Flask等Web框架集成，可以很方便的在Web开发中处理一些异步任务。

Celery本身并没有提供消息队列的功能，他的所有任务信息需要使用其他工具来存储任务信息。

Celery整体可以分为4个模块：

任务模块(Tasks)：包括异步任务和定时任务，异步任务在程序中处罚，定时任务由Celery Beat调度。

消息中间件(Brokers)：真正存储任务信息的地方，常见的Brokers有rabbitmq、redis等。

任务执行单元(Workers)：从任务队列取出任务并执行。

结果存储(Backend/Result Stores)：存储任务执行结果的地方。支持AMQP, Redis，memcached, MongoDB，SQLAlchemy, Django ORM等。

整体架构如下图所示：

![celery](https://shengjieli.github.io/img/weixin/celery.png)

### 安装

`pip install celery`

也可以同时安装redis客户端：
`pip install celery[redis]`

### 异步任务

来看下Celery的异步任务是如何使用的，broker和backend都使用redis来存储。

```python
# tasks.py
from celery import Celery

BROKER = 'redis://localhost:6379/0'
BACKEND = 'redis://localhost:6379/1'

app = Celery('tasks', broker=BROKER, backend=BACKEND)

@app.task
def add(x, y):
    return x + y
```

创建一个Celery实例，并指定broker和backend，然后定义一个简单的函数并用app.task装饰器装饰，函数就可以成为一个Celery的异步任务了。

然后在命令行运行：`celery worker -A tasks -l info`

worker表示我们启动的是Celery的worker，-A参数是指定要到哪个模块下去找Celery的实例，-l参数指定日志级别。

然后新开一个终端，在IPython中运行下边的代码就可以添加一个任务了。
```
In [1]: from tasks import add
In [2]: task = add.delay(1, 2)
In [3]: print(task.ready())  # 任务结果是否准备好
True
In [4]: print(task.status)  # 任务状态
SUCCESS
In [5]: print(task.result)  # 任务执行结果
3
In [6]: print(task.get())  # 阻塞直到任务完成
3
In [7]: print(task.id)  # 任务id
'd82ffe8b-b121-4db1-8cb6-7030404d5a46'
```

可以在Celery worker的终端中看到worker收到任务的日志和任务完成的日志。

在配置的backend的redis中使用`keys *`命令可以看到有一个名字为celery-task-meta-后边加上任务id的key，使用type可以看出这是一个string类型的值，用get看一下里边存储了什么：

```
127.0.0.1:6379[1]> keys *
1) "celery-task-meta-d82ffe8b-b121-4db1-8cb6-7030404d5a46"
127.0.0.1:6379[1]> type "celery-task-meta-c3356c2f-6640-4464-88a0-5eb36121d6b2"
string
127.0.0.1:6379[1]> type "celery-task-meta-d82ffe8b-b121-4db1-8cb6-7030404d5a46"
string
127.0.0.1:6379[1]> get "celery-task-meta-d82ffe8b-b121-4db1-8cb6-7030404d5a46"
"{\"status\": \"SUCCESS\", \"task_id\": \"d82ffe8b-b121-4db1-8cb6-7030404d5a46\", \"date_done\": \"2019-07-13T16:57:05.389108\", \"traceback\": null, \"result\": 3, \"children\": []}"
```

可以看出Celery把任务的状态、id、完成时间、出错信息，结果等信息都以json的形式存储到了redis中。

*Celery的结果序列化方式也是可以配置的*

还可以把配置文件单独放到一个文件里：

config.py
```python
BROKER_URL = 'redis://127.0.0.1:6379/0'
CELERY_RESULT_BACKEND = 'redis://127.0.0.1:6379/1'

CELERY_TIMEZONE='Asia/Shanghai'
```

tasks.py
```python
from celery import Celery

app = Celery('tasks')
app.config_from_object('config')

@app.task
def add(x, y):
    return x + y
```

之前都是用delay函数调用，delay函数其实是调用了apply_async函数，apply_async函数支持更多的参数：

```python
In [1]: from tasks import add
In [2]: task = add.apply_async(args=(1, 2), countdown=20)  # 20秒后执行
In [3]: print(task.ready())  # 任务结果是否准备好
False
# 20秒后
In [4]: print(task.ready())
True
In [5]: print(task.result)  # 任务执行结果
3
In [6]: import datetime
In [7]: tasks.add.apply_async(args=[1, 3], eta=datetime.datetime.now() + datetime.timedelta(seconds=10))  # 在指定的时间运行
Out[7]: <AsyncResult: d7c936ff-eeae-439f-972a-f112326b3708>
In [8]: tasks.add.apply_async(args=[1, 3], expires=10)  # 10秒后过期
Out[9]: <AsyncResult: 801bcb4b-1bec-4818-86a6-f261b56128a8>
```

还可以给任务绑定为task实例的方法，这样在方法中可以获取到任务的一些信息：

tasks.py
```python
# coding: utf-8
from celery import Celery

app = Celery('tasks')
app.config_from_object('config')

@app.task(bind=True)  # 用实例方法绑定
def add(self, x, y):
    print("request args: ", self.request.args)  # 获取任务参数
    return x + y
```

我们也可以自定义task类并重写一些方法：

tasks.py
```python
from celery import Celery
from celery.app.task import Task

app = Celery('tasks')
app.config_from_object('config')


class MyTask(Task):
    def on_success(self, retval, task_id, args, kwargs):
        print 'task {} success.'.format(task_id)
        return super(MyTask, self).on_success(retval, task_id, args, kwargs)
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        print 'task {} fail, exc: {}'.format(task_id, exc)
        return super(MyTask, self).on_failure(exc, task_id, args, kwargs, einfo)


@app.task(base=MyTask)
def div(x, y):
    return x / y
```

如果任务执行成功会调用on_success方法，失败则会调用on_failure方法，例如：

在IPython中：

```python
In [1]: from tasks import div

In [2]: div.delay(4, 2)
Out[2]: <AsyncResult: a00e7d83-3f1d-4643-ad22-1f5aa0d35f67>

In [3]: div.delay(4, 0)
Out[3]: <AsyncResult: 77b289a4-ad73-46a8-b7af-9dfea7332f9b>
```

Celery worker就会打印出如下日志：

```python
[2019-07-14 15:52:44,675: INFO/MainProcess] Received task: tasks.div[a00e7d83-3f1d-4643-ad22-1f5aa0d35f67]  
[2019-07-14 15:52:44,688: WARNING/ForkPoolWorker-1] task a00e7d83-3f1d-4643-ad22-1f5aa0d35f67 success.
[2019-07-14 15:52:44,689: INFO/ForkPoolWorker-1] Task tasks.div[a00e7d83-3f1d-4643-ad22-1f5aa0d35f67] succeeded in 0.0116934529506s: 2
[2019-07-14 15:52:48,346: INFO/MainProcess] Received task: tasks.div[77b289a4-ad73-46a8-b7af-9dfea7332f9b]  
[2019-07-14 15:52:48,349: WARNING/ForkPoolWorker-1] task 77b289a4-ad73-46a8-b7af-9dfea7332f9b fail, exc: integer division or modulo by zero
[2019-07-14 15:52:48,349: ERROR/ForkPoolWorker-1] Task tasks.div[77b289a4-ad73-46a8-b7af-9dfea7332f9b] raised unexpected: ZeroDivisionError('integer division or modulo by zero',)
Traceback (most recent call last):
  File "/home/conf/venv/celery_py3/lib/python2.7/site-packages/celery/app/trace.py", line 385, in trace_task
    R = retval = fun(*args, **kwargs)
  File "/home/conf/venv/celery_py3/lib/python2.7/site-packages/celery/app/trace.py", line 648, in __protected_call__
    return self.run(*args, **kwargs)
  File "/home/conf/test/celery_demo/celery_app/tasks.py", line 20, in div
    return x / y
ZeroDivisionError: integer division or modulo by zero
```

### 定时任务

Celery还支持定时任务，可以在配置文件中配置定时任务执行的时间或者间隔。

tasks.py
```python
from celery import Celery

app = Celery('tasks')
app.config_from_object('config')

@app.task
def beat_task(a, b):
    return a * b
```

config.py
```python
from datetime import timedelta
from celery.schedules import crontab

BROKER_URL = 'redis://127.0.0.1:6379/0'
CELERY_RESULT_BACKEND = 'redis://127.0.0.1:6379/1'

CELERY_TIMEZONE='Asia/Shanghai'

CELERYBEAT_SCHEDULE = {
    'beat_task': {
        'task': 'tasks.beat_task',
        'schedule': timedelta(seconds=30),  # 每隔30秒执行一次
        'args': (4, 3)  # 参数
    },
    'crontab_task': {
        'task': 'tasks.beat_task',
        'schedule': crontab(hour=9, minute=10),
        'args': (5, 6)
    }
}
```

首先启动worker：
`celery worker -A tasks -l info`

然后还要启动beat，beat会定时向celery添加任务：
`celery beat -A tasks`

这样每隔30秒会执行一次参数为(4, 3)的beat_task，每天9:10会执行一次参数为(5, 6)的beat_task。

celery的worker和beat还可以一起启动:
`celery worker -B -A tasks -l info`


### 分布式Celery

Celery也是支持分布式的，可以在多台机器上启动多个worker，还可以配置某个任务发给哪个worker来执行。

tasks.py
```python
from celery import Celery
from celery.app.task import Task

app = Celery('tasks')
app.config_from_object('config')

@app.task
def add(x, y):
    return x + y

@app.task
def sub(x, y):
    return x - y
```

config.py
```python
from datetime import timedelta
from celery.schedules import crontab
from kombu import Exchange, Queue

BROKER_URL = 'redis://127.0.0.1:6379/0'
CELERY_RESULT_BACKEND = 'redis://127.0.0.1:6379/1'

CELERY_TIMEZONE='Asia/Shanghai'

CELERY_QUEUES = (
    Queue("task_add_queue",Exchange("add_exchange"),routing_key="task_add"),
    Queue("task_sub_queue",Exchange("sub_exchange"),routing_key="task_sub")
)

CELERY_ROUTES = {
    'tasks.add':{"queue":"task_add_queue","routing_key":"task_add"},
    'tasks.sub':{"queue":"task_sub_queue","routing_key":"task_sub"}
}
```

client.py
```python
from tasks import add, sub

add.delay(1, 3)
sub.delay(3, 2)
```

分别启动task_add_queue和task_sub_queue的worker：
`celery worker -A tasks -l info -n add -Q task_add_queue`
`celery worker -A tasks -l info -n sub -Q task_sub_queue`

然后执行client.py，就可以看到add函数被发给了task_add_queue的队列运行，sub函数被发给了task_sub_queue队列运行。

关于Celery，就先介绍这些，感兴趣的话可以查看官方文档更深入的学习Celery。
