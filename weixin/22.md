# 用asycnio和aiohttp实现一个并发爬虫


上篇文章介绍了Python的asycnio并发编程。今天，我们来用asyncio和aiohttp来实现一个并发爬虫，看下asyncio有多强大。

aiohttp是基于asycnio的一个异步http客户端和服务器的库。作为客户端，它的使用方式和requests很相似。

今天，我们来用asyncio和aiohttp来实现一个并发爬虫抓取某公司招聘网站，并把数据存入MySQL数据库中。由于我们要使用异步编程，所以，关于MySQL的相关操作会使用支持异步操作的aiomysql来实现。

首先通过chrome浏览器的开发者工具分析发现此网站的数据全是通过XMLHttpRequest获取的，根据这个请求的url和参数，就可以用aiohttp来构造我们的请求获取数据。通过改变pageIndex参数来实现翻页，根据获取到响应的字段在MySQL中新建一张表：

```sql
CREATE TABLE `alibaba` (
  `id` int(11) NOT NULL,
  `applyed` int(11) DEFAULT NULL,
  `code` varchar(20) DEFAULT NULL,
  `degree` varchar(20) DEFAULT NULL,
  `departmentId` int(11) DEFAULT NULL,
  `departmentName` varchar(200) DEFAULT NULL,
  `description` text,
  `effectiveDate` bigint(11) DEFAULT NULL,
  `expired` int(11) DEFAULT NULL,
  `favorited` int(11) DEFAULT NULL,
  `firstCategory` varchar(50) DEFAULT NULL,
  `gmtCreate` bigint(11) DEFAULT NULL,
  `gmtModified` bigint(11) DEFAULT NULL,
  `isNew` char(1) DEFAULT NULL,
  `isOpen` char(1) DEFAULT NULL,
  `isUrgent` char(1) DEFAULT NULL,
  `name` varchar(200) DEFAULT NULL,
  `recruitNumber` int(11) DEFAULT NULL,
  `requirement` text,
  `secondCategory` varchar(50) DEFAULT NULL,
  `status` varchar(20) DEFAULT NULL,
  `uneffectualDate` bigint(11) DEFAULT NULL,
  `workExperience` varchar(20) DEFAULT NULL,
  `workLocation` varchar(50) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci
```

之后我们开始写代码实现这个并发爬虫：

```python
import asyncio

import aiohttp
import aiomysql


mysql_host = "127.0.0.1"
mysql_user = ""
mysql_password = ""

url = "https://job.alibaba.com/zhaopin/socialPositionList/doList.json"
headers = {
    "referer": "https://job.alibaba.com/zhaopin/positionList.htm",
    "x-requested-with": "XMLHttpRequest",
    "content-type": "application/x-www-form-urlencoded; charset=UTF-8",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36"
}
sql = """insert into alibaba(applyed,code,degree,departmentId,departmentName,description,effectiveDate,expired,favorited,firstCategory,gmtCreate,gmtModified,id,isNew,isOpen,isUrgent,name,recruitNumber,requirement,secondCategory,status,uneffectualDate,workExperience,workLocation) value(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"""


async def save(pool, data):
    """
    将数据批量存储到MySQL
    """
    d = [(each["applyed"],each["code"],each["degree"],each["departmentId"],each["departmentName"],each["description"],each["effectiveDate"],each["expired"],each["favorited"],each["firstCategory"],each["gmtCreate"],each["gmtModified"],each["id"],each["isNew"],each["isOpen"],each["isUrgent"],each["name"],each["recruitNumber"],each["requirement"],each["secondCategory"],each["status"],each["uneffectualDate"],each["workExperience"],each["workLocation"]) for each in data]
    async with pool.acquire() as conn:
        async with conn.cursor() as cur:
            try:
                ret = await cur.executemany(sql, d)
            except Exception as e:
                print(e)
                return


async def aio_post(session, pool, url, params):
    """
    根据url和参数发送post请求
    """
    async with session.post(url, headers=headers, data=params) as response:
        res = await response.json()
        data = res["returnValue"]["datas"]
        await save(pool, data)


async def crawl(session, pool):
    """
    根据页数创建抓取任务
    """
    tasks = []
    params = "pageSize=10&t=0.9576984861702043&pageIndex={}&keyWord=&location=&second=&first="
    for page in range(1, 613):  # 手动获取页数，当然也可以根据第一页返回的页数自动翻页
        # 此处一定要使用create_task，不能直接await，否则不能并发运行
        task = asyncio.create_task(aio_post(session, pool, url, params.format(page)))
        tasks.append(task)
    # 等待所有任务执行完毕
    for task in tasks:
        await task


async def main():
    session = aiohttp.ClientSession()
    loop = asyncio.get_event_loop()
    pool = await aiomysql.create_pool(host=mysql_host, port=3306,
                                      user=mysql_user, password=mysql_password,
                                      db='job', loop=loop, autocommit=True)
    await crawl(session, pool)
    await session.close()
    pool.close()
    await pool.wait_closed()


if __name__ == "__main__":
    asyncio.run(main())

```

```shell
$ time python ali_test.py 

real	0m5.154s
user	0m1.739s
sys	    0m0.119s
```

运行一下发现只需要13秒就发送了600多个请求，实际上如果把存储MySQL的过程注释掉只需要5秒就可以完成600多个请求，存储MySQL这个过程耗费这么长的时间暂时不知道时我的使用方式不对还是这个库本身的问题或是其他原因。

既然拿到了这么多数据，肯定要做一波分析了，在后边的文章中会再写写关于数据的分析和可视化。
